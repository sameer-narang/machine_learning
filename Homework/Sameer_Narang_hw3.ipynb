{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d42635940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "1.1 Dataset construction\n",
    "'''\n",
    "\"\"\"\n",
    "Generating training, test, validation splits for the Lasso homework.\n",
    "\n",
    "Author: David Rosenberg <david.davidr@gmail.com>\n",
    "License: Creative Commons Attribution 4.0 International License\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def stepFnGenerator(stepLoc=0):\n",
    "    def f(x):\n",
    "        ret = np.zeros(len(x))\n",
    "        ret[x >= stepLoc] = 1\n",
    "        return ret\n",
    "    return f\n",
    "\n",
    "def linearCombGenerator(fns, coefs):\n",
    "    def f(x):\n",
    "        return sum(fns[i](x) * coefs[i] for i in range(len(fns)))\n",
    "    return f\n",
    "\n",
    "def get_data(n=1000, num_basis_fns = 100, num_nonzero = 10, noise_SD=.25):\n",
    "    # We'll construct a function as a linear combination of step \"basis\"\n",
    "    # functions. We'll then apply it randomly chosen points from interval [0,1]\n",
    "    # to create data. We'll featurize the points from the input space [0,1]\n",
    "    # using our basis functions.\n",
    "\n",
    "    # Construct basis, to be used for generating target function and features\n",
    "    all_basis_fns = [stepFnGenerator(stepLoc=s)\n",
    "                     for s in np.linspace(0, 1, num_basis_fns, endpoint=False)]\n",
    "\n",
    "    # Construct target function (the Bayes prediction function)\n",
    "    nonzero_indices = np.random.choice (num_basis_fns, num_nonzero)\n",
    "    coefs = np.zeros (num_basis_fns)\n",
    "    coefs[nonzero_indices] = np.random.randn (num_nonzero)\n",
    "    np.count_nonzero ()\n",
    "    target_fn = linearCombGenerator(all_basis_fns, coefs)\n",
    "\n",
    "    # Construct dataset\n",
    "    x = np.sort(np.random.rand(n))\n",
    "    y_target = target_fn(x)\n",
    "    y_observed = y_target + noise_SD * np.random.randn(n)\n",
    "\n",
    "    # Featurize input values in []0,1]\n",
    "    X_ftrs = np.empty((n, num_basis_fns))\n",
    "    for ftr_num in range(num_basis_fns):\n",
    "        X_ftrs[:, ftr_num] = all_basis_fns[ftr_num](x)\n",
    "\n",
    "    return x, X_ftrs, y_observed, target_fn\n",
    "\n",
    "def get_data_splits(n=1000, num_basis_fns=100, num_nonzero=10,\n",
    "                    noise_SD=.25, test_frac=.2):\n",
    "\n",
    "    x, X_ftrs, y_observed, target_fn = get_data(n, num_basis_fns,\n",
    "                                                num_nonzero, noise_SD)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_ftrs, y_observed,\n",
    "                                                        test_size=test_frac,\n",
    "                                                        random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=1)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def main_fn_data_gen ():\n",
    "    # Generate data and write it to disk\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = get_data_splits()\n",
    "    np.savez_compressed('data.npz', X_train=X_train, y_train=y_train,\n",
    "                        X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test)\n",
    "\n",
    "    # Demonstrate loading data back\n",
    "    data = np.load('data.npz')\n",
    "    X_train = data['X_train']\n",
    "    X_val = data['X_val']\n",
    "    y_train = data['y_train']\n",
    "    y_val = data['y_val']\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "\n",
    "    # Let's plot the target function (i.e. the Bayes prediction function) as\n",
    "    # well as some noisy observations, as a function of the original input\n",
    "    # space, which is the interval [0,1]. The data actually returned in\n",
    "    # X_train, etc. is a featurization of this input space using step\n",
    "    # functions.  See code above.\n",
    "    x, X_ftrs, y_observed, target_fn = get_data()\n",
    "    y_Bayes = target_fn(x)\n",
    "    ## Plot target function\n",
    "    plt.scatter(x,y_observed, color='r')\n",
    "    plt.plot(x,y_Bayes)\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "X_train = None\n",
    "X_val = None\n",
    "y_train = None\n",
    "y_val = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "'''\n",
    "main_fn_data_gen()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (\"Size: X_train - \" + str (X_train.size ()) + \", X_val - \" + str (X_val) + \", X_test - \" + str (X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoping this is very close to 0:  5.73664688349e-12\n",
      "Number of non-0 coefficients with l2 regularization (own model): 100\n",
      "Number of non-0 coefficients with l2 regularization (skklearn): 100\n",
      "    param_l2reg  mean_test_score  mean_train_score\n",
      "0      0.000001         0.068757          0.046626\n",
      "1      0.000010         0.068757          0.046626\n",
      "2      0.000100         0.068756          0.046626\n",
      "3      0.001000         0.068752          0.046626\n",
      "4      0.010000         0.068710          0.046627\n",
      "5      0.100000         0.068433          0.046736\n",
      "13     0.230000         0.068380          0.047126\n",
      "14     0.460000         0.068876          0.048199\n",
      "15     0.690000         0.069759          0.049506\n",
      "16     0.920000         0.070823          0.050901\n",
      "6      1.000000         0.071215          0.051393\n",
      "21     1.080000         0.071614          0.051884\n",
      "17     1.150000         0.071967          0.052314\n",
      "22     1.160000         0.072017          0.052375\n",
      "25     1.181000         0.072124          0.052504\n",
      "26     1.202000         0.072231          0.052632\n",
      "27     1.223000         0.072337          0.052760\n",
      "23     1.240000         0.072424          0.052864\n",
      "28     1.244000         0.072444          0.052888\n",
      "29     1.265000         0.072551          0.053016\n",
      "30     1.286000         0.072659          0.053143\n",
      "24     1.320000         0.072832          0.053349\n",
      "18     1.380000         0.073139          0.053711\n",
      "19     1.610000         0.074312          0.055076\n",
      "20     1.840000         0.075472          0.056401\n",
      "9      2.000000         0.076266          0.057298\n",
      "10     4.000000         0.085141          0.066967\n",
      "11     6.000000         0.092448          0.074677\n",
      "12     8.000000         0.098766          0.081270\n",
      "7     10.000000         0.104419          0.087149\n",
      "8    100.000000         0.245735          0.227194\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ridge regression using scipy's minimize function and demonstrating the use of\n",
    "sklearn's framework.\n",
    "\n",
    "Author: David Rosenberg <david.davidr@gmail.com>\n",
    "License: Creative Commons Attribution 4.0 International License\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "class RidgeRegression(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" ridge regression\"\"\"\n",
    "\n",
    "    def __init__(self, l2reg=1):\n",
    "        if l2reg < 0:\n",
    "            raise ValueError('Regularization penalty should be at least 0.')\n",
    "        self.l2reg = l2reg\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        n, num_ftrs = X.shape\n",
    "        # convert to 1-dim array, in case we're given a column vector\n",
    "        y = y.reshape(-1) \n",
    "        def ridge_obj(w):\n",
    "            predictions = np.dot(X,w)\n",
    "            residual = y - predictions\n",
    "            empirical_risk = np.sum(residual**2)\n",
    "            l2_norm_squared = np.sum(w**2)\n",
    "            objective = empirical_risk + self.l2reg * l2_norm_squared\n",
    "            return objective\n",
    "        self.ridge_obj_ = ridge_obj\n",
    "\n",
    "        w_0 = np.zeros(num_ftrs)\n",
    "        self.w_ = minimize(ridge_obj, w_0).x\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"w_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "\n",
    "        return np.dot(X, self.w_)\n",
    "\n",
    "\n",
    "def main_fn_ridge_reg ():\n",
    "\n",
    "    # Get data\n",
    "    load_data_from_file = True\n",
    "    if load_data_from_file:\n",
    "        data = np.load('data.npz')\n",
    "        X_train = data['X_train']\n",
    "        X_val = data['X_val']\n",
    "        y_train = data['y_train']\n",
    "        y_val = data['y_val']\n",
    "        X_test = data['X_test']\n",
    "        y_test = data['y_test']\n",
    "        #print (\"Size: X_train - \" + str (X_train.size) + \", X_val - \" + str (X_val.size) + \", X_test - \" + str (X_test.size))\n",
    "    else:\n",
    "        from generate_data import get_data_splits\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test = get_data_splits(n=1000, num_basis_fns=100, num_nonzero=10)\n",
    "\n",
    "    # Compare our RidgeRegression to sklearn's.\n",
    "\n",
    "    # First run sklearn ridge regression and extract the coefficients\n",
    "    from sklearn.linear_model import Ridge\n",
    "    l2_reg = 2\n",
    "    # Fit with sklearn -- need to multiply l2_reg by sample size, since their\n",
    "    # objective function is the total square loss, rather than average square\n",
    "    # loss.\n",
    "    n = X_train.shape[0]\n",
    "    sklearn_ridge = Ridge(alpha=l2_reg, fit_intercept=False, normalize=False)\n",
    "    sklearn_ridge.fit(X_train, y_train) \n",
    "    sklearn_ridge_coefs = sklearn_ridge.coef_\n",
    "\n",
    "    # Now run our ridge regression and compare the coefficients to sklearn's\n",
    "    ridge_regression_estimator = RidgeRegression(l2reg=l2_reg)\n",
    "    ridge_regression_estimator.fit(X_train, y_train)\n",
    "    our_coefs = ridge_regression_estimator.w_\n",
    "\n",
    "    print (\"Hoping this is very close to 0: \", np.sum( \\\n",
    "        (our_coefs - sklearn_ridge_coefs)**2 ))\n",
    "    print (\"Number of non-0 coefficients with l2 regularization (own model): \" + \\\n",
    "           str (np.count_nonzero (our_coefs)))\n",
    "    print (\"Number of non-0 coefficients with l2 regularization (sklearn): \" + \\\n",
    "           str (np.count_nonzero (sklearn_ridge_coefs)))\n",
    "    \n",
    "    # Now let's use sklearn to help us do hyperparameter searching\n",
    "    from sklearn.model_selection import GridSearchCV,PredefinedSplit\n",
    "    from sklearn.model_selection import ParameterGrid\n",
    "    from sklearn.metrics import mean_squared_error, make_scorer\n",
    "    import pandas as pd\n",
    "\n",
    "    # grid.fit by default splits the data into training and\n",
    "    # validation itself; we want to use our own splits, so we need to stack our\n",
    "    # training and validation sets together, and supply an index\n",
    "    # (validation_fold) to specify which entries are train and which are\n",
    "    # validation.\n",
    "    X_train_val = np.vstack((X_train, X_val))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "    val_fold = [-1]*len(X_train) + [0]*len(X_val) #0 corresponds to validation\n",
    "\n",
    "    # Now we set up and do the grid search over l2reg the np.concatenate\n",
    "    # command illustrates my search for the best hyperparameter. In each line,\n",
    "    # I'm zooming in to a particular hyperparameter range that showed promise\n",
    "    # in the previous grid. This approach works reasonably well when\n",
    "    # performance is convex as a function of the hyperparameter, which it seems\n",
    "    # to be here.\n",
    "    param_grid = [{'l2reg':np.concatenate((10.**np.arange(-6,3,1),\n",
    "                                           np.arange(2,10,2),\n",
    "                                           np.arange(.23,2,.23),\n",
    "                                           np.arange(1.08,1.38,.08),\n",
    "                                           np.arange(1.181,1.3,.021)\n",
    "                                             )) }]\n",
    "    ridge_regression_estimator = RidgeRegression()\n",
    "    grid = GridSearchCV(ridge_regression_estimator,\n",
    "                        param_grid,\n",
    "                        cv = PredefinedSplit(test_fold=val_fold),\n",
    "                        scoring = make_scorer(mean_squared_error,\n",
    "                                              greater_is_better = False))\n",
    "    grid.fit(X_train_val, y_train_val) \n",
    "\n",
    "    #    pd.set_option('display.max_rows', 20)\n",
    "    df = pd.DataFrame(grid.cv_results_)\n",
    "    # Flip sign of score back, because GridSearchCV likes to maximize,\n",
    "    # so it flips the sign of the score if \"greater_is_better=FALSE\"\n",
    "    df['mean_test_score'] = -df['mean_test_score']\n",
    "    df['mean_train_score'] = -df['mean_train_score']\n",
    "    cols_to_keep = [\"param_l2reg\", \"mean_test_score\",\"mean_train_score\"]\n",
    "    df_toshow = df[cols_to_keep].fillna('-')\n",
    "    df_toshow = df_toshow.sort_values(by=[\"param_l2reg\"])\n",
    "    print (df_toshow)\n",
    "\n",
    "main_fn_ridge_reg ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.33333333,  0.66666667])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.linspace(0, 1, 3, endpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3 & 3.4 - Implement Lasso regression - implemented using matrix operations as well as a for loop - shooting algo\n",
    "'''\n",
    "\n",
    "class LassoRegression(BaseEstimator, RegressorMixin):\n",
    "    \"\"\" ridge regression\"\"\"\n",
    "\n",
    "    def __init__(self, l1reg=1):\n",
    "        if l1reg < 0:\n",
    "            raise ValueError('Regularization penalty should be at least 0.')\n",
    "        self.l1reg = l1reg\n",
    "\n",
    "    def get_threshold (self, x, lambda_val):\n",
    "        if x > 0 and lambda_val < abs (x):\n",
    "            return x - lambda_val\n",
    "        elif x < 0 and lambda_val < abs (x):\n",
    "            return x + lambda_val\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def slow_fit (self, X, y, num_iter=1000, theta=None):\n",
    "        num_obs, num_ftrs = X.shape\n",
    "        \n",
    "        init_theta_flag = False\n",
    "        if not theta:\n",
    "            if init_theta_flag:\n",
    "                self.theta = np.dot (np.dot (np.linalg.inv (np.dot (X.transpose (), X) + \\\n",
    "                             self.l1reg * np.identity (num_ftrs)), X.transpose ()), y)\n",
    "            else:\n",
    "                self.theta = np.zeros (num_ftrs)\n",
    "        else:\n",
    "            self.theta = theta\n",
    "            \n",
    "        converged = False\n",
    "        prev_loss = 0\n",
    "        while (converged):\n",
    "            for j in range (num_ftrs):\n",
    "                a_j = 2 * np.sum (X [:,j] ** 2)\n",
    "                c_j = None\n",
    "                for i in range (num_obs):\n",
    "                    c_j = X [i,j] * (y [i] - np.dot (self.theta.transpose (), X [i,:]) + \\\n",
    "                          self.theta [j] * X [i] [j])\n",
    "                    self.theta [j] = self.get_threshold (c_j/a_j, self.l1reg/a_j)\n",
    "                loss = compute_square_loss (X_train, y_train, self.theta)\n",
    "            if (prev_loss == loss):\n",
    "                converged = True\n",
    "            prev_loss = loss\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    # sn - using the optimized version with matrix operations instead of loops over \n",
    "    # individual elements\n",
    "    def fit(self, X, y, num_iter=1000, theta=None):\n",
    "        num_obs, num_ftrs = X.shape\n",
    "        \n",
    "        init_theta_flag = False\n",
    "        if not theta:\n",
    "            if init_theta_flag:\n",
    "                self.theta = np.dot (np.dot (np.linalg.inv (np.dot (X.transpose (), X) + \\\n",
    "                             self.l1reg * np.identity (num_ftrs)), X.transpose ()), y)\n",
    "            else:\n",
    "                self.theta = np.zeros (num_ftrs)\n",
    "        else:\n",
    "            self.theta = theta\n",
    "        for i in range (num_iter):\n",
    "            for j in range (num_ftrs):\n",
    "                theta_inc = deepcopy (self.theta)\n",
    "                theta_inc [j] = 0\n",
    "                j_residual = y - np.dot (X, theta_inc)\n",
    "                arg1 = np.dot (X [:, j], j_residual)\n",
    "                arg2 = self.l1reg * num_obs\n",
    "                self.theta [j] = self.get_threshold (arg1, arg2)/(X[:, j]**2).sum ()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        try:\n",
    "            getattr(self, \"w_\")\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(\"You must train classifer before predicting data!\")\n",
    "\n",
    "        return np.dot(X, self.w_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    loss = 0 #initialize the square_loss\n",
    "    L = np.dot (X, theta) - y\n",
    "    loss = np.dot (L, L)/ (2 * y.size)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHmBHA0kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d3aa7c898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-loss with lambda = 0.1: 0.1565599192\n",
      "Number of non-0 coefficients with l1 regularization: 9\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3.1 - plot square loss against lambda\n",
    "'''\n",
    "\n",
    "data = np.load('data.npz')\n",
    "X_train = data['X_train']\n",
    "X_val = data['X_val']\n",
    "y_train = data['y_train']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "#lr_model = LassoRegression (l1reg=1)\n",
    "#lr_model.fit (X_train, y_train)\n",
    "\n",
    "lambda_vals_to_test = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "theta_vector = {}\n",
    "loss_vector = []\n",
    "for lval in lambda_vals_to_test:\n",
    "    lr_model = LassoRegression (l1reg=lval)\n",
    "    lr_model.fit (X_train, y_train)\n",
    "    loss_vector.append (compute_square_loss (X_val, y_val, lr_model.theta))\n",
    "    theta_vector [lval] = lr_model.theta\n",
    "    \n",
    "plt.plot (np.log10 (lambda_vals_to_test), loss_vector)\n",
    "plt.show ()\n",
    "print (\"Test-loss with lambda = 0.1: \" + str (compute_square_loss (X_test, y_test, \\\n",
    "       theta_vector [0.1])))\n",
    "\n",
    "print (\"Number of non-0 coefficients with l1 regularization: \" + \\\n",
    "       str (np.count_nonzero (theta_vector [0.1])))\n",
    "#lr_model.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n3.2 - sparsity of the solution.As seen above, the number of non-zero coefficients is seen as 9 against the actual/ true non-zero coefficienet count of 10\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "3.2 - sparsity of the solution.\n",
    "As seen above, the number of non-zero coefficients is seen as 9 \n",
    "against the actual/ true non-zero coefficienet count of 10\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3.4 - optimized algorithm with matrix operations implemented above\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4.1 (1) - One sided directional derivative of the Lasso objective function\n",
    "\n",
    "The lasso objective function \n",
    "L(w) = (Transpose (X.w -y)).(X.w - y) + lamba * first_order_norm (w)\n",
    "-- expression A\n",
    "\n",
    "Using first principles, \n",
    "L (w + vh) = (Transpose (X.(w + vh) - y).(X.(w + vh) - y)) + \n",
    "                  lambda * first_order_norm (w + vh)\n",
    "-- expresision B\n",
    "\n",
    "where h is a \"small\" positive scalar approaching 0 under the limit\n",
    "\n",
    "On exapanding expressions A and B and taking the difference, we get\n",
    "\n",
    "L (w + hv) - L (w) = \n",
    "h * [Transpose (w).Transpose (X).X.v + Transpose (v).Transpose (X).X.w + \n",
    "    h * Transpose (v).Transpose (X).X.v\n",
    "    - Transpose (v).Transpose (X).y - Transpose (y).X.v] + \n",
    "    lambda * first_order_norm (w + vh) - lambda * first_order_norm (w)\n",
    "                     \n",
    "Dividing by h as per the definition of the directional derivative and \n",
    "applying the limit h approaches 0, we get:\n",
    "\n",
    "L_prime (w, v) = Transpose (w).Transpose (X).X.v + Transpose (v).Transpose (X).X.w \n",
    "                 - Transpose (v).Transpose (X).y - Transpose (y).X.v + \n",
    "                 lambda * first_order_norm (w + vh) - lambda * first_order_norm (w)\n",
    "\n",
    "At w=0, we get\n",
    "\n",
    "L_prime (0, v) = \n",
    "lambda * first_order_norm (v) - Transpose (v).Transpose (X).y - Transpose (y).X.v\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4.1 (2) Lower bound for lambda\n",
    "\n",
    "Using the expression for L_prime (0, v) derived above,\n",
    "\n",
    "Clearly, under the condition that v <> 0 i.e. first_order_norm (v) <> 0,\n",
    "\n",
    "L_prime (0, v) >= 0 => \n",
    "lambda >= (Transpose (v).Transpose (X).y + Transpose (y).X.v) / first_order_norm (v)\n",
    "-- expression C\n",
    "\n",
    "Thus, C = (Transpose (v).Transpose (X).y + Transpose (y).X.v) / first_order_norm (v)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4.1 (3) - Maximum lower bound\\n\\nExpression C from above can be rewritten as:\\n\\n(Transpose (v).(Transpose (X).y) + Transpose (Transpose (X).y).v) / first_order_norm (v)\\n\\nBoth terms Transpose (v).(Transpose (X).y) and Transpose (Transpose (X).y).v) are transpose of each other and are scalars\\nso we can rewrite it as\\n\\n2 * Transpose (v). (Transpose (X).y) / first_order_norm (v)\\n\\nSince v is a unit direction vector, the above expression is maximized when exactly 1 element is 1 - placing the maximum\\nweight on the corresponding element from Transpose (X).y, and the maximum value achieved is thus\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4.1 (3) - Maximum lower bound\n",
    "\n",
    "Expression C from above can be rewritten as:\n",
    "\n",
    "(Transpose (v).(Transpose (X).y) + Transpose (Transpose (X).y).v) / first_order_norm (v)\n",
    "\n",
    "Both terms Transpose (v).(Transpose (X).y) and Transpose (Transpose (X).y).v) \n",
    "are transpose of each other and are scalars so we can rewrite it as\n",
    "\n",
    "2 * Transpose (v). (Transpose (X).y) / first_order_norm (v)\n",
    "\n",
    "Since v is a unit direction vector, \n",
    "the above expression is maximized when exactly 1 element is 1 - placing the maximum\n",
    "weight on the corresponding element from Transpose (X).y, and the maximum value achieved is thus\n",
    "\n",
    "2 * max_element_of (Transpose (X).y) \n",
    "--- immediately recognized as the supremum norm\n",
    "\n",
    "i.e.\n",
    "\n",
    "2 * supremum_norm (Transpose (X).y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
