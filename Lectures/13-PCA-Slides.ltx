\title{Lecture 13}
\subtitle{Principal Component Analysis}
\begin{document}
\begin{frame} 
  \titlepage 
\end{frame}
\section{Lecture 13}
\subsection{Initial Question}
\begin{frame}
  \frametitle{Intro Question}
  \begin{block}{Question}
    Let $S\in\RR^{n\times n}$ be symmetric.
    \begin{enumerate}
    \item How does $\trace{S}$ relate to the spectral
      decomposition $S=W\Lambda W^T$ where $W$ is orthogonal and $\Lambda$
      is diagonal?
    \item How do you solve $w_* = \argmax_{\|w\|_2=1} w^TSw$?  What is $w_*^TSw_*$?
    \end{enumerate}
  \end{block}
\end{frame}
\begin{frame}
  \frametitle{Intro Solution}
  \begin{block}{Solution}
    \begin{enumerate}
    \item We use the following useful property of traces:
      $\trace{AB}=\trace{BA}$ for any matrices $A,B$ where the
      dimensions allow.  Thus we have
      $$\trace{S} = \trace{W(\Lambda W^T)} = \trace{(\Lambda W^T)W} =
      \trace{\Lambda},$$
      so the trace of $S$ is the sum of its eigenvalues.
    \item $w_*$ is an eigenvector with the largest eigenvalue.  Then
      $w_*^TSw_*$ is the largest eigenvalue.
    \end{enumerate}
  \end{block}
\end{frame}
\subsection{Principal Component Analysis (PCA)}
\begin{frame}
  \frametitle{Unsupervised Learning}
  \begin{enumerate}
  \item Where did the $y$'s go?
  \item Try to find intrinsic structure in unlabeled data.
  \item With PCA, we are looking for a low dimensional affine subspace
    that approximates our data well.
  \end{enumerate}
\end{frame}
\subsection{Definition of Principal Components}
\begin{frame}
  \frametitle{Centered Data}
  \begin{enumerate}
  \item Throughout this lecture we will work with centered data.
  \item Suppose $X\in\RR^{n\times d}$ is our data matrix.  Define
    $$\overline{x} = \frac{1}{n}\sum_{i=1}^n x_i.$$
  \item Let $\overline{X}\in\RR^{n\times d}$ be the matrix with
    $\overline{x}$ in every row.
  \item Define the centered data:
    $$\tilde{X} = X-\overline{X},\quad \tilde{x}_i = x_i-\overline{x}.$$
  \end{enumerate}
\end{frame}
\begin{frame}
\frametitle{Variance Along A Direction}
\begin{block}{Definition}
  Let $\tilde{x}_1,\ldots,\tilde{x}_n\in\RR^d$ be the centered data.
  Fix a direction $w\in\RR^d$ with $\|w\|_2=1$.  The sample variance
  along $w$ is given by
  $$\frac{1}{n-1}\sum_{i=1}^n (\tilde{x}_i^Tw)^2.$$
  This is the sample variance of the components
  $$\tilde{x}_1^Tw,\ldots,\tilde{x}_n^Tw.$$
\end{block}
\begin{enumerate}
\item This is also the sample variance of
 $$x_1^Tw,\ldots,x_n^Tw,$$
  using the uncentered data.
\end{enumerate}
\end{frame}
\begin{frame}
\frametitle{Variance Along A Direction}
\centering
\asyinclude[height=.8\textheight]{13-PCA/Project0.asy}
\end{frame}
\begin{frame}
\frametitle{Variance Along A Direction}
\centering
\asyinclude[height=.8\textheight]{13-PCA/Project.asy}
\end{frame}
\begin{frame}
\frametitle{First Principal Component}
\begin{enumerate}
\item Define the first loading vector $w_{(1)}$ to be the direction
  giving the highest variance:
  $$w_{(1)} = \argmax_{\|w\|_2=1}\frac{1}{n-1}\sum_{i=1}^n
  (\tilde{x}_i^Tw)^2.$$
\item Maximizer is not unique, so we choose one.
\end{enumerate}
\begin{block}{Definition}
  The \textit{first principal component} of $\tilde{x}_i$ is $\tilde{x}_i^Tw_{(1)}$.
\end{block}
\end{frame}
\begin{frame}
\frametitle{Principal Components}
\begin{enumerate}
\item Define the $k$th loading vector $w_{(k)}$ to be the direction
  giving the highest variance that is orthogonal to the first $k-1$
  loading vectors:
  $$w_{(k)} = \argmax_{\substack{\|w\|_2=1\\w\perp w_{(1)},\ldots,w_{(k-1)}}}\frac{1}{n-1}\sum_{i=1}^n
  (\tilde{x}_i^Tw)^2.$$
\item The complete set of loading vectors $w_{(1)},\ldots,w_{(d)}$
  form an orthonormal basis for $\RR^d$.
\end{enumerate}
\begin{block}{Definition}
  The \textit{$k$th principal component} of $\tilde{x}_i$ is $\tilde{x}_i^Tw_{(k)}$.
\end{block}
\end{frame}
\begin{frame}
\frametitle{Principal Components}
\begin{enumerate}
\item Let $W$ denote the matrix with the $k$th loading vector
  $w_{(k)}$ as its $k$th column.
\item Then $W^T\tilde{x}_i$ gives the principal components of
  $\tilde{x}_i$ as a column vector.
\item $\tilde{X}W$ gives a new data matrix in terms of principal
  components.
\item If we compute the singular value decomposition (SVD) of $\tilde{X}$ we
  get
  $$\tilde{X} = VDW^T,$$
  where $D\in\RR^{n\times d}$ is diagonal with non-negative entries,
  and $V\in\RR^{n\times n}$, $W\in\RR^{d\times d}$ are orthogonal.
\item Then $\tilde{X}^T\tilde{X} = WD^TDW^T$.  Thus we can use
  the SVD on our data matrix to obtain the loading vectors $W$ and the
  eigenvalues $\Lambda = \frac{1}{n-1}D^TD$.
\end{enumerate}
\end{frame}
\subsection{Computing Principal Components}
\begin{frame}
\frametitle{Some Linear Algebra}
Recall that $w_{(1)}$ is defined by
$$w_{(1)}=\argmax_{\|w\|_2=1}\frac{1}{n-1}\sum_{i=1}^n
(\tilde{x}_i^Tw)^2.$$
We now perform some algebra to simplify this expression.
Note that
$$\begin{Array}{rcl}
  \sum_{i=1}^n (\tilde{x}_i^Tw)^2
  & = & \sum_{i=1}^n (\tilde{x}_i^Tw)(\tilde{x}_i^Tw)\\
  & = & \sum_{i=1}^n (w^T\tilde{x}_i)(\tilde{x}_i^Tw)\\
  & = & w^T\left[\sum_{i=1}^n \tilde{x}_i\tilde{x}_i^T\right]w\\
  & = & w^T\tilde{X}^T\tilde{X}w.
\end{Array}$$
\end{frame}
\begin{frame}
\frametitle{Some Linear Algebra}
\begin{enumerate}
\item This shows
$$w_{(1)} = \argmax_{\|w\|_2=1}\frac{1}{n-1}w^T\tilde{X}^T\tilde{X}w =
\argmax_{\|w\|_2=1}w^TSw,$$
where $S=\frac{1}{n-1}\tilde{X}^T\tilde{X}$ is the sample covariance
matrix.
\item By the introductory problem this implies $w_{(1)}$ is the
  eigenvector corresponding to the largest eigenvalue of $S$.
\item We also learn that the variance along $w_{(1)}$ is $\lambda_1$,
  the largest eigenvalue of $S$.
\item With a bit more work we can see that $w_{(k)}$ is the
  eigenvector corresponding to the $k$th largest eigenvalue, with
  $\lambda_k$ giving the associated variance.
\end{enumerate}
\end{frame}
\begin{frame}
\frametitle{PCA Example}
\begin{block}{Example}
  A collection of people come to a testing site to have their heights
  measured twice.  The two testers use different measuring devices,
  each of which introduces errors into the measurement process.  Below
  we depict some of the measurements computed (already centered).
\end{block}
\end{frame}
\begin{frame}
\frametitle{PCA Example}
\begin{center}
\asyinclude[height=.6\textheight]{13-PCA/Meas.asy}
\end{center}
  \begin{enumerate}
  \item Describe (vaguely) what you expect the sample covariance matrix to look like.
  \item What do you think $w_{(1)}$ and $w_{(2)}$ are?
  \end{enumerate}
\end{frame}
\begin{frame}
\frametitle{PCA Example: Solutions}
\begin{enumerate}
\item We expect tester 2 to have a larger variance than tester 1, and
  to be nearly perfectly correlated.  The sample covariance matrix is
  $$S = \pMatt{40.5154&   93.5069}{ 93.5069  & 232.8653}.$$
\item We have
  $$\hspace{-.5cm}S = W\Lambda W^T, W = \pMatt{0.3762 & -0.9265}{0.9265 &
  0.3762}, \Lambda = \diagg{270.8290}{2.5518}.$$
\end{enumerate}
\begin{itemize}
\item Note that $\trace{\Lambda}=\trace{S}$.
\item Since $\lambda_2$ is small, it shows that $w_{(2)}$ is
  almost in the null space of $S$.  This suggests
  $-.9265\tilde{x}^1+.3762\tilde{x}^2\approx 0$ for data points $(\tilde{x}^1,\tilde{x}^2)$.  In other
  words, $\tilde{x}^2\approx 2.46\tilde{x}^1$.  Maybe tester 2 used centimeters and
  tester 1 used inches.
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{PCA Example: Plot In Terms of Principal Components}
\centering
\asyinclude[height=.6\textheight]{13-PCA/MeasP.asy}
\asyinclude[height=.3\textheight]{13-PCA/MeasP2.asy}
\end{frame}
\begin{frame}
\subsection{Uses of PCA}
\frametitle{Uses of PCA: Dimensionality Reduction}
\begin{enumerate}
\item In our height example above, we can replace
  our two features with only a single feature, the first principal
  component.  
\item This can be used as a preprocessing step in a supervised
  learning algorithm.
\item When performing dimensionality reduction, one must choose how many
  principal components to use.  This is often done using a scree plot: a
  plot of the eigenvalues of $S$ in descending order.
\item Often people look for an ``elbow'' in the scree plot: a point where
  the plot becomes much less steep.
\end{enumerate}
\end{frame}
\begin{frame}
\frametitle{Scree Plot}
\includegraphics[height=.8\textheight]{13-PCA/JoliffeScree.png}
\footnotetext[1]{From Jolliffe, Principal Component Analysis}
\end{frame}
\begin{frame}
\frametitle{Uses of PCA: Visualization}
\begin{enumerate}
\item Visualization: If we have high dimensional data, it can be hard
  to plot it effectively.  Sometimes plotting the first two principal
  components can reveal interesting geometric structure in
  the data.
\end{enumerate}
\centering
\includegraphics[height=.6\textheight]{13-PCA/Europe.jpg}
\footnotetext[1]{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2735096/}
\end{frame}
\begin{frame}
\frametitle{Uses of PCA: Principal Component Regression}
\begin{enumerate}
\item Want to build a linear model with a dataset
  $$\DD=\{(x_1,y_1),\ldots,(x_n,y_n)\}.$$
\item We can choose some $k$ and replace each $\tilde{x}_i$ with its first $k$ principal
  components.  Afterward we perform linear regression.  
\item This is called principal component regression, and can be thought of as a discrete
  variant of ridge regression (see HTF 3.4.1).
\item Correlated features may be grouped together into a single
  principal component that averages their values (like with ridge
  regression).  Think about the 2 tester example from before.
\end{enumerate}
\end{frame}
\subsection{Other Comments About PCA}
\begin{frame}
\frametitle{Standardization}
\begin{enumerate}
\item What happens if you scale one of the features by a huge factor?
\pause
\item It will have a huge variance and become a dominant part of the
  first principal component.
\item To add scale-invariance to the process, people often standardize
  their data (center and normalize) before running PCA.
\item This is the same as using the correlation matrix in place of the
  covariance matrix.
\end{enumerate}
\end{frame}
\begin{frame}
\frametitle{Dispersion Of The Data}
\begin{enumerate}
\item One measure of how dispersed our data is the following:
  $$\Delta = \frac{1}{n-1}\sum_{i=1}^n \|x_i-\overline{x}\|_2^2 = \frac{1}{n-1}\sum_{i=1}^n
  \|\tilde{x}_i\|_2^2.$$
\item A little algebra shows this is $\trace{S}$, where $S$ is the
  sample covariance matrix.
\item If we project onto the first $k$ principal components, the
  resulting data has dispersion $\lambda_1+\cdots+\lambda_k$.
\item We can choose $k$ to account for a desired percentage of $\Delta$.
\item The subspace spanned by the first $k$ loading vectors maximizes
  the resulting dispersion over all possible $k$-dimensional
  subspaces.
\end{enumerate}
\end{frame}
\begin{frame}
\frametitle{Other Comments}
\begin{enumerate}
\item The $k$-dimensional subspace $V$ spanned by $w_{(1)},\ldots,w_{(k)}$
  best fits the centered data in the least-squares sense.  More precisely, it minimizes
  $$\sum_{i=1}^n \|x_i-P_V(x_i)\|_2^2$$
  over all $k$-dimensional subspaces, 
  where $P_V$ orthogonally projects onto $V$.
\item Converting your data into principal components can sometimes hurt
  interpretability since the new features are linear combinations
  (i.e., blends or baskets) of your old features.
\item The smallest principal components, if they correspond to small
  eigenvalues, are nearly in the null space of $X$, and thus can reveal
  linear dependencies in the centered data.
\end{enumerate}
\end{frame}
\begin{frame}
\frametitle{Principal Components Are Linear}
Suppose we have the following labeled data.
\begin{center}
\asyinclude[height=.65\textheight]{13-PCA/Annuli.asy}
\end{center}
How can we apply PCA and obtain a single principal component that
distinguishes the colored clusters?
\end{frame}
\begin{frame}
\frametitle{Principal Components Are Linear}
\begin{enumerate}
\item In general, can deal with non-linear by adding features or using
  kernels.
\item Using kernels results in the technique called Kernel PCA.
\item Below we added the feature $\|\tilde{x}_i\|^2$ and took the
  first principal component.
\end{enumerate}
\begin{center}
\asyinclude{13-PCA/Annuli2.asy}
\end{center}
\end{frame}
\end{document}
